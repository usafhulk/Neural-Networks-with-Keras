{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l15URzqUE1Ac"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N2Th76_uE12R"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas==2.2.3) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas==2.2.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas==2.2.3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas==2.2.3) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.16.0)\n",
      "Collecting tensorflow==2.17.1\n",
      "  Downloading tensorflow-2.17.1-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.17.1 (from tensorflow==2.17.1)\n",
      "  Downloading tensorflow_intel-2.17.1-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (3.11.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.1->tensorflow==2.17.1) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\chris\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (13.7.1)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading optree-0.15.0-cp312-cp312-win_amd64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.1->tensorflow==2.17.1) (0.1.0)\n",
      "Downloading tensorflow-2.17.1-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.17.1-cp312-cp312-win_amd64.whl (382.4 MB)\n",
      "   ---------------------------------------- 0.0/382.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/382.4 MB 8.4 MB/s eta 0:00:46\n",
      "    --------------------------------------- 5.0/382.4 MB 11.2 MB/s eta 0:00:34\n",
      "    --------------------------------------- 8.4/382.4 MB 12.7 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 12.6/382.4 MB 14.3 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 16.8/382.4 MB 15.5 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 22.0/382.4 MB 16.8 MB/s eta 0:00:22\n",
      "   -- ------------------------------------- 27.8/382.4 MB 18.4 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 33.8/382.4 MB 19.5 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 38.8/382.4 MB 19.9 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 44.8/382.4 MB 20.7 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 50.9/382.4 MB 21.3 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 57.4/382.4 MB 22.2 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 62.9/382.4 MB 22.4 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 70.5/382.4 MB 23.3 MB/s eta 0:00:14\n",
      "   ------- -------------------------------- 76.3/382.4 MB 24.1 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 80.0/382.4 MB 23.1 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 85.5/382.4 MB 23.2 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 92.5/382.4 MB 23.6 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 97.3/382.4 MB 23.6 MB/s eta 0:00:13\n",
      "   ---------- ---------------------------- 104.3/382.4 MB 24.0 MB/s eta 0:00:12\n",
      "   ----------- --------------------------- 113.0/382.4 MB 24.3 MB/s eta 0:00:12\n",
      "   ------------ -------------------------- 119.5/382.4 MB 24.5 MB/s eta 0:00:11\n",
      "   ------------ -------------------------- 124.5/382.4 MB 24.5 MB/s eta 0:00:11\n",
      "   ------------- ------------------------- 131.1/382.4 MB 24.8 MB/s eta 0:00:11\n",
      "   -------------- ------------------------ 137.9/382.4 MB 25.0 MB/s eta 0:00:10\n",
      "   -------------- ------------------------ 143.1/382.4 MB 25.1 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 150.2/382.4 MB 25.3 MB/s eta 0:00:10\n",
      "   --------------- ----------------------- 156.5/382.4 MB 25.5 MB/s eta 0:00:09\n",
      "   ---------------- ---------------------- 163.8/382.4 MB 25.7 MB/s eta 0:00:09\n",
      "   ----------------- --------------------- 171.4/382.4 MB 26.0 MB/s eta 0:00:09\n",
      "   ------------------ -------------------- 180.9/382.4 MB 26.6 MB/s eta 0:00:08\n",
      "   ------------------ -------------------- 182.7/382.4 MB 26.0 MB/s eta 0:00:08\n",
      "   ------------------- ------------------- 191.9/382.4 MB 26.5 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 197.4/382.4 MB 26.6 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 198.7/382.4 MB 25.9 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 205.8/382.4 MB 26.0 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 211.0/382.4 MB 26.0 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 216.0/382.4 MB 26.0 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 216.5/382.4 MB 25.9 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 220.7/382.4 MB 25.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 225.7/382.4 MB 25.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 230.9/382.4 MB 25.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 232.0/382.4 MB 24.7 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 236.5/382.4 MB 24.7 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 240.4/382.4 MB 24.5 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 244.3/382.4 MB 24.3 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 248.5/382.4 MB 24.2 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 252.4/382.4 MB 24.1 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 256.6/382.4 MB 24.0 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 260.8/382.4 MB 23.9 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 265.0/382.4 MB 24.1 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 269.2/382.4 MB 24.3 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 272.9/382.4 MB 24.3 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 276.3/382.4 MB 24.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 280.5/382.4 MB 24.1 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 284.4/382.4 MB 24.0 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 288.9/382.4 MB 24.0 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 293.1/382.4 MB 23.7 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 296.7/382.4 MB 23.6 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 300.9/382.4 MB 23.5 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 305.1/382.4 MB 23.3 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 309.3/382.4 MB 23.3 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 313.5/382.4 MB 23.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 317.7/382.4 MB 23.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 322.4/382.4 MB 22.9 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 326.9/382.4 MB 22.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 331.1/382.4 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 335.3/382.4 MB 22.3 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 339.5/382.4 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 344.2/382.4 MB 22.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 348.1/382.4 MB 22.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 352.3/382.4 MB 22.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 357.0/382.4 MB 22.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 361.8/382.4 MB 22.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 365.7/382.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 370.1/382.4 MB 21.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  374.9/382.4 MB 21.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  379.3/382.4 MB 21.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.2/382.4 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.2/382.4 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.2/382.4 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.2/382.4 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.2/382.4 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.2/382.4 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.2/382.4 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.2/382.4 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  382.2/382.4 MB 21.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 382.4/382.4 MB 18.5 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---------------------------------------  4.2/4.3 MB 20.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 15.1 MB/s eta 0:00:00\n",
      "Downloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 13.7 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 4.7/26.4 MB 22.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 8.4/26.4 MB 19.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 12.6/26.4 MB 19.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.8/26.4 MB 19.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 21.0/26.4 MB 20.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.9/26.4 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.4 MB 19.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 16.6 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 3.9/5.5 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 16.0 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.15.0-cp312-cp312-win_amd64.whl (307 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.2.2 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 keras-3.9.2 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.9 opt-einsum-3.4.0 optree-0.15.0 tensorboard-2.17.1 tensorboard-data-server-0.7.2 tensorflow-2.17.1 tensorflow-intel-2.17.1 termcolor-3.1.0\n",
      "Collecting pillow==11.1.0\n",
      "  Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.0/2.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pillow\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "Successfully installed pillow-11.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.1.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.9.2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Chris\\anaconda3\\Lib\\site-packages\\~atplotlib'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.8/7.8 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.1/7.8 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.5/7.8 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.8 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 10.3 MB/s eta 0:00:00\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.9.3\n",
      "    Uninstalling matplotlib-3.9.3:\n",
      "      Successfully uninstalled matplotlib-3.9.3\n",
      "Successfully installed matplotlib-3.9.2\n",
      "Collecting transformers==4.38.2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (2.32.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
      "  Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers==4.38.2) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.38.2) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.2) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers==4.38.2) (2025.1.31)\n",
      "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.6/8.5 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.9/8.5 MB 10.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.6/8.5 MB 12.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.5/8.5 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 15.3 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.48.3\n",
      "    Uninstalling transformers-4.48.3:\n",
      "      Successfully uninstalled transformers-4.48.3\n",
      "Successfully installed tokenizers-0.15.2 transformers-4.38.2\n",
      "Requirement already satisfied: torch in c:\\users\\chris\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==2.2.3\n",
    "!pip install tensorflow==2.17.1\n",
    "!pip install pillow==11.1.0\n",
    "!pip install matplotlib==3.9.2\n",
    "!pip install transformers==4.38.2\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Jd0EKRQfE3tn"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.applications import VGG16\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "PduMrEddE5ZP"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bgIo-JtGtFB"
   },
   "source": [
    "# Part 1: Classifying the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46lz1dacGzKA"
   },
   "source": [
    "#### 1.1 Dataset Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "eo7-NoGnFI-F"
   },
   "outputs": [],
   "source": [
    "#Set the batch size,epochs\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "img_rows, img_cols = 224, 224\n",
    "input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\Chris\\\\OneDrive\\\\IBM AI Engineering\\\\Jupyter Notebooks\\\\Course 2 Deep Learing and Neural Networks with Keras\\\\aircraft_damage_dataset_v1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Now download the tar file using the URL from the text file\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlretrieve(url, tar_filename)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtar_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Extraction will begin now.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Check if the folder already exists\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:250\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Handle temporary file setup.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m--> 250\u001b[0m     tfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m     tfp \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\Chris\\\\OneDrive\\\\IBM AI Engineering\\\\Jupyter Notebooks\\\\Course 2 Deep Learing and Neural Networks with Keras\\\\aircraft_damage_dataset_v1'"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Local file paths\n",
    "filepath = r\"C:\\Users\\Chris\\OneDrive\\IBM AI Engineering\\Jupyter Notebooks\\Course 2 Deep Learing and Neural Networks with Keras\\dataset_url.txt\"\n",
    "tar_filename = r\"C:\\Users\\Chris\\OneDrive\\IBM AI Engineering\\Jupyter Notebooks\\Course 2 Deep Learing and Neural Networks with Keras\\aircraft_damage_dataset_v1\"\n",
    "extracted_folder = \"aircraft_damage_dataset_v1\"  # Folder where contents will be extracted\n",
    "\n",
    "# Read the URL from the text file instead of treating the file path as a URL\n",
    "with open(filepath, 'r') as file:\n",
    "    url = file.read().strip()\n",
    "    \n",
    "# Now download the tar file using the URL from the text file\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(url, tar_filename)\n",
    "print(f\"Downloaded {tar_filename}. Extraction will begin now.\")\n",
    "\n",
    "# Check if the folder already exists\n",
    "if os.path.exists(extracted_folder):\n",
    "    print(f\"The folder '{extracted_folder}' already exists. Removing the existing folder.\")\n",
    "\n",
    "    # Remove the existing folder to avoid overwriting or duplication\n",
    "    shutil.rmtree(extracted_folder)\n",
    "    print(f\"Removed the existing folder: {extracted_folder}\")\n",
    "\n",
    "# Extract the contents of the tar file\n",
    "with tarfile.open(tar_filename, \"r\") as tar_ref:\n",
    "    tar_ref.extractall()  # This will extract to the current directory\n",
    "    print(f\"Extracted {tar_filename} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "A2CV2gT2FWJb"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unknown url type: 'OneDrive - SNHU/IBM AI Engineering/Jupyter Notebooks/Course 2 Deep Learing and Neural Networks with Keras/dataset_url.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m extracted_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maircraft_damage_dataset_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Folder where contents will be extracted\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Download the tar file\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlretrieve(filepath, tar_filename)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtar_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Extraction will begin now.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Check if the folder already exists\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:240\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(urlopen(url, data)) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    241\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:499\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, fullurl, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39msocket\u001b[38;5;241m.\u001b[39m_GLOBAL_DEFAULT_TIMEOUT):\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;66;03m# accept a URL or a Request object\u001b[39;00m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fullurl, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 499\u001b[0m         req \u001b[38;5;241m=\u001b[39m Request(fullurl, data)\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m         req \u001b[38;5;241m=\u001b[39m fullurl\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:318\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[1;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    316\u001b[0m              origin_req_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, unverifiable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    317\u001b[0m              method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_url \u001b[38;5;241m=\u001b[39m url\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munredirected_hdrs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:344\u001b[0m, in \u001b[0;36mRequest.full_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url \u001b[38;5;241m=\u001b[39m unwrap(url)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfragment \u001b[38;5;241m=\u001b[39m _splittag(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url)\n\u001b[1;32m--> 344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:373\u001b[0m, in \u001b[0;36mRequest._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype, rest \u001b[38;5;241m=\u001b[39m _splittype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_url)\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 373\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown url type: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_url)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselector \u001b[38;5;241m=\u001b[39m _splithost(rest)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost:\n",
      "\u001b[1;31mValueError\u001b[0m: unknown url type: 'OneDrive - SNHU/IBM AI Engineering/Jupyter Notebooks/Course 2 Deep Learing and Neural Networks with Keras/dataset_url.txt'"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import urllib.request\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# URL of the tar file\n",
    "filepath = \"OneDrive - SNHU/IBM AI Engineering/Jupyter Notebooks/Course 2 Deep Learing and Neural Networks with Keras/dataset_url.txt\"\n",
    "\n",
    "# Define the path to save the file\n",
    "tar_filename = \"OneDrive - SNHU/IBM AI Engineering/Jupyter Notebooks/Course 2 Deep Learing and Neural Networks with Keras/aircraft_damage_dataset_v1.tar\"\n",
    "extracted_folder = \"aircraft_damage_dataset_v1\"  # Folder where contents will be extracted\n",
    "\n",
    "# Download the tar file\n",
    "urllib.request.urlretrieve(filepath, tar_filename)\n",
    "print(f\"Downloaded {tar_filename}. Extraction will begin now.\")\n",
    "\n",
    "# Check if the folder already exists\n",
    "if os.path.exists(extracted_folder):\n",
    "    print(f\"The folder '{extracted_folder}' already exists. Removing the existing folder.\")\n",
    "\n",
    "    # Remove the existing folder to avoid overwriting or duplication\n",
    "    shutil.rmtree(extracted_folder)\n",
    "    print(f\"Removed the existing folder: {extracted_folder}\")\n",
    "\n",
    "# Extract the contents of the tar file\n",
    "with tarfile.open(tar_filename, \"r\") as tar_ref:\n",
    "    tar_ref.extractall()  # This will extract to the current directory\n",
    "    print(f\"Extracted {tar_filename} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "au9KiBH2FWD_"
   },
   "outputs": [],
   "source": [
    "# showing the content of the folder\n",
    "\n",
    "import os\n",
    "\n",
    "def print_directory_structure(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f'{indent}{os.path.basename(root)}/')\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f'{subindent}{f}')\n",
    "\n",
    "# Assuming 'extracted_folder' holds the name of the extracted dataset directory\n",
    "extracted_folder = \"aircraft_damage_dataset_v1\"\n",
    "print_directory_structure(extracted_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wm95BswlFWBh"
   },
   "outputs": [],
   "source": [
    "# Define directories for train, test, and validation splits\n",
    "extract_path = \"aircraft_damage_dataset_v1\"\n",
    "train_dir = os.path.join(extract_path, 'train')\n",
    "test_dir = os.path.join(extract_path, 'test')\n",
    "valid_dir = os.path.join(extract_path, 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsobvTaIG5SH"
   },
   "source": [
    "#### 1.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBD5_kEBFV0N"
   },
   "outputs": [],
   "source": [
    "# Create ImageDataGenerators to preprocess the data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xb6gQ-5_G-dT"
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_rows, img_cols),   # Resize images to the size VGG16 expects\n",
    "    batch_size=batch_size,\n",
    "    seed = seed_value,\n",
    "    class_mode='binary',\n",
    "    shuffle=True # Binary classification: dent vs crack\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rU3WPJ2HEE2"
   },
   "source": [
    "## **Task 1: Create a `valid_generator` using the `valid_datagen` object**\n",
    "\n",
    "\n",
    "Please use the following parameters:\n",
    "\n",
    "*   **directory** should be set to `valid_dir`.\n",
    "*   **class_mode** should be set to `'binary'`.\n",
    "*   **seed** should be set to `seed_value`.\n",
    "*   **batch_size** should be set to `batch_size`.\n",
    "*   **shuffle** should be set to `False`.\n",
    "*   **target_size** should be set to `(img_rows, img_cols)`.\n",
    "\n",
    "Hint: the format should be like:\n",
    "\n",
    "```python\n",
    "valid_generator =  valid_datagen.flow_from_directory(\n",
    "    directory=,\n",
    "    class_mode=,\n",
    "    seed=,\n",
    "    batch_size=,\n",
    "    shuffle=,\n",
    "    target_size=\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sj-jiOnHllB"
   },
   "outputs": [],
   "source": [
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    directory = valid_dir,\n",
    "    class_mode = 'binary',\n",
    "    seed = seed_value,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    target_size = (img_rows, img_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSBdZiw4IPf8"
   },
   "source": [
    "## **Task 2: Create a `test_generator` using the `test_datagen` object**\n",
    "\n",
    "Please use the following parameters:\n",
    "\n",
    "*   **directory** should be set to `test_dir`.\n",
    "*   **class_mode** should be set to `'binary'`.\n",
    "*   **seed** should be set to `seed_value`.\n",
    "*   **batch_size** should be set to `batch_size`.\n",
    "*   **shuffle** should be set to `False`.\n",
    "*   **target_size** should be set to `(img_rows, img_cols)`.\n",
    "\n",
    "Hint: The format should be like:\n",
    "\n",
    "```python\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=,\n",
    "    class_mode=,\n",
    "    seed=,\n",
    "    batch_size=,\n",
    "    shuffle=,\n",
    "    target_size=\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpfVKf9XIa0Z"
   },
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory = test_dir,\n",
    "    class_mode = 'binary',\n",
    "    seed = seed_value,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    target_size = (img_rows, img_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJ6u_vGYJY4R"
   },
   "source": [
    "## **Task 3: Load the pre-trained model VGG16**\n",
    "\n",
    "Set <code>weights='imagenet'</code>,<code>include_top=False</code>,<code>input_shape=(img_rows, img_cols, 3)</code>\n",
    "\n",
    "Hint: The format should be like:\n",
    "\n",
    "base_model = VGG16(weights= , include_top= , input_shape=)\n",
    "\n",
    "****Note: Please copy and save the code of the task as it will be required for submission in the final project. Ensure to submit the response as part of your project submission****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrzfmNcCJ0iq"
   },
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "721nvY-BKIkt"
   },
   "outputs": [],
   "source": [
    "output = base_model.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "base_model = Model(base_model.input, output)\n",
    "\n",
    "# Freeze the base VGG16 model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYR7G1HdKWlc"
   },
   "outputs": [],
   "source": [
    "# Build the custom model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "my8GLjsIKaKj"
   },
   "source": [
    "## **Task 4: Compile the model**\n",
    "\n",
    "You will compile the model using the following parameters:\n",
    "\n",
    "*   **loss**: `'binary_crossentropy'`.\n",
    "*   **optimizer**: `=Adam(learning_rate=0.0001)`.\n",
    "*   **metrics**: `['accuracy']`.\n",
    "\n",
    "Hint: Use `model.compile()` to compile the model:\n",
    "    \n",
    "```python\n",
    "model.compile(\n",
    "    optimizer=,\n",
    "    loss=,\n",
    "    metrics=\n",
    ")\n",
    "```\n",
    "**Note: Please copy and save the code of the task as it will be required for submission in the final project. Ensure to submit the response as part of your project submission.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1l2biVHTKgdF"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-5_fsbXLAkv"
   },
   "source": [
    "## Task 5: Model Training\n",
    "Now that the model is compiled, you can train it using the .fit() method. This step involves passing in the training and validation datasets along with the number of epochs you want to train the model for.\n",
    "\n",
    "You will train the model using the following parameters:\n",
    "\n",
    "*   **train_data**: `train_generator`\n",
    "*   **epochs**: `n_epochs`\n",
    "*   **validation_data**: `valid_generator`\n",
    "\n",
    "Hint: Use `model.fit()` to train the model:\n",
    "    \n",
    "```python\n",
    "history = model.fit(\n",
    "    <train_data>,  # Fill in with the training data generator or dataset\n",
    "    epochs=<number_of_epochs>,  # Fill in with the number of epochs for training\n",
    "    validation_data=<validation_data>,  # Fill in with the validation data generator or dataset\n",
    "   \n",
    ")\n",
    "\n",
    "```\n",
    "**Note: Please copy and save the code of the task as it will be required for submission in the final project. Ensure to submit the response as part of your project submission**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QPJjMw3ALC5j"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      2\u001b[0m     train_generator,\n\u001b[0;32m      3\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mn_epochs,\n\u001b[0;32m      4\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalid_generator,\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=n_epochs,\n",
    "    validation_data=valid_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tvkI-aZLOz3"
   },
   "outputs": [],
   "source": [
    "train_history = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vvQzlP_QTx2"
   },
   "outputs": [],
   "source": [
    "# Plot the loss for both training and validation\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(train_history['loss'])\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(train_history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms4v_vlxT5ou"
   },
   "source": [
    "## Task 6: Plot accuracy curves for training and validation sets\n",
    "\n",
    "Hint: Similar to the loss curves. Use `plt.plot()` to plot the accuracy curves for training and validation sets.\n",
    "\n",
    "- `figsize=(5, 5)`\n",
    "- `plt.plot(train_history['accuracy'], label='Training Accuracy')`\n",
    "- `plt.plot(train_history['val_accuracy'], label='Validation Accuracy')`\n",
    "- **Title**: `'Accuracy Curve'`\n",
    "- **xlabel**: `'Epochs'`\n",
    "- **ylabel**: `'Accuracy'`\n",
    "\n",
    "**Note: Please copy and save the code of the task as it will be required for submission in the final project. Ensure to submit the response as part of your project submission**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5O0GFV1hT-KN"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(train_history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(train_history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OnBzJqd9T_lh"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_generator, steps\u001b[38;5;241m=\u001b[39mtest_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m test_generator\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2z7ey5qUi6M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Function to plot a single image and its prediction\n",
    "def plot_image_with_title(image, model, true_label, predicted_label, class_names):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Convert labels from one-hot to class indices if needed, but for binary labels it's just 0 or 1\n",
    "    true_label_name = class_names[true_label]  # Labels are already in class indices\n",
    "    pred_label_name = class_names[predicted_label]  # Predictions are 0 or 1\n",
    "\n",
    "    plt.title(f\"True: {true_label_name}\\nPred: {pred_label_name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Function to test the model with images from the test set\n",
    "def test_model_on_image(test_generator, model, index_to_plot=0):\n",
    "    # Get a batch of images and labels from the test generator\n",
    "    test_images, test_labels = next(test_generator)\n",
    "\n",
    "    # Make predictions on the batch\n",
    "    predictions = model.predict(test_images)\n",
    "\n",
    "    # In binary classification, predictions are probabilities (float). Convert to binary (0 or 1)\n",
    "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "    # Get the class indices from the test generator and invert them to get class names\n",
    "    class_indices = test_generator.class_indices\n",
    "    class_names = {v: k for k, v in class_indices.items()}  # Invert the dictionary\n",
    "\n",
    "    # Specify the image to display based on the index\n",
    "    image_to_plot = test_images[index_to_plot]\n",
    "    true_label = test_labels[index_to_plot]\n",
    "    predicted_label = predicted_classes[index_to_plot]\n",
    "\n",
    "    # Plot the selected image with its true and predicted labels\n",
    "    plot_image_with_title(image=image_to_plot, model=model, true_label=true_label, predicted_label=predicted_label, class_names=class_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8MvdaVNUnrc"
   },
   "source": [
    "## **Task 7: Visualizing the results**\n",
    "\n",
    "In this task, you will specify which image from the test dataset to display and test the model on. You will fill in the required values to test your model.\n",
    "\n",
    "You will visualize using the following parameters:\n",
    "\n",
    "*   **test_data_generator**: `test_generator`.\n",
    "*   **model**: `model`.\n",
    "*   **index_to_plot**: `1`.\n",
    "\n",
    "Hint: Use `test_model_on_image` to visualize the result:\n",
    "    \n",
    "```python\n",
    "test_model_on_image(<test_data_generator>, <model>, index_to_plot=index_to_plot)\n",
    "\n",
    "```\n",
    "**NOTE**: Due to the inherent nature of neural networks, predictions may vary from the actual labels. For instance, if the actual label is ‘crack’, the prediction could be either ‘crack’ or ‘dent’, both of which are possible outcomes, and full marks will be awarded for the task.\n",
    "\n",
    "**Note: Please copy and save the code of the task as it will be required for submission in the final project. Ensure to submit the response as part of your project submission**                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HB79Rv8KUrJw"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_model_on_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m index_to_plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 2\u001b[0m test_model_on_image(test_generator, model, index_to_plot\u001b[38;5;241m=\u001b[39mindex_to_plot)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_model_on_image' is not defined"
     ]
    }
   ],
   "source": [
    "index_to_plot = 1\n",
    "test_model_on_image(test_generator, model, index_to_plot=index_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J11eyWydVIMi"
   },
   "source": [
    "## <a id='toc2_'></a>[Part 2: Image Captioning and Summarization using BLIP Pretrained Model](#toc2_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srVUQcZkVJOZ"
   },
   "source": [
    "BLIP (Bootstrapping Language-Image Pretraining) is an advanced vision-and-language model designed to generate natural language descriptions for images. By leveraging both visual and textual information, BLIP can produce human-readable text that accurately reflects the content and context of an image. It is specifically trained to understand images and their relationships to summarizing text, making it ideal for tasks like image captioning, summarization, and visual question answering.\n",
    "\n",
    "In this project, learners will utilize the BLIP model to build a system capable of automatically generating captions and summary for images. The code will integrate the BLIP model within a custom Keras layer. This allows the user to input an image and specify a task, either \"caption\" or \"summary\", to receive a textual output that describes or summarizes the content of the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU53RPqXVLRz"
   },
   "source": [
    "#### Key Steps:\n",
    "\n",
    "- **Image Loading and Preprocessing:** The code will begin by loading images from a file path, then converting and processing them into a format suitable for input to the BLIP model.\n",
    "  \n",
    "- **Text Generation:** Depending on the task, whether generating a caption or summary, the BLIP model will generate corresponding text based on the processed image.\n",
    "  \n",
    "- **Custom Keras Layer:** A custom Keras layer is a user-defined layer that extends Keras' built-in functionality.Here custom Keras layer will be implemented to wrap the BLIP model. This layer will handle the task-specific processing (captioning or summarizing) and integrate smoothly into a TensorFlow/Keras environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7FuVbJiyUzST"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Load the required libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BlipProcessor, BlipForConditionalGeneration\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#Load the required libraries\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nga3yX5GVWcb"
   },
   "source": [
    "## 2.1 Loading BLIP Model\n",
    "\n",
    "#### Load the BLIP Model and Processor from Hugging Face\n",
    "\n",
    "Hugging Face is an open-source platform that provides pre-trained machine learning models, datasets, and tools, primarily focused on natural language processing, computer vision, and other AI tasks. It offers easy access to powerful models through its Transformers library.\n",
    "\n",
    "- **BlipProcessor:** This handles the preprocessing of images and text. It converts images to the format that the BLIP model can understand.\n",
    "\n",
    "- **BlipForConditionalGeneration:** This is the model itself, responsible for generating captions or summaries based on the processed image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "LItsvRfuVQi6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BlipProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#load the pretrained BLIP processor and model:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m processor \u001b[38;5;241m=\u001b[39m BlipProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/blip-image-captioning-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m BlipForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/blip-image-captioning-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BlipProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "#load the pretrained BLIP processor and model:\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1ks6FviVj74"
   },
   "source": [
    "#### Custom Keras Layer: BlipCaptionSummaryLayer\n",
    "\n",
    "Next, we define a custom `tf.keras.layers.Layer` class that takes in an image and a task input (either caption or summary) and processes the image using the BLIP model. To create a custom Keras layer, we need to subclass `tf.keras.layers.Layer` and implement the required methods.\n",
    "\n",
    "#### **BlipCaptionSummaryLayer Class:**\n",
    "\n",
    "This custom Keras layer integrates image preprocessing and text generation using a pretrained BLIP model.\n",
    "\n",
    "- **`__init__`**: This constructor method initializes the `BlipCaptionSummaryLayer` class by setting up the BLIP processor and model.\n",
    "  \n",
    "- **`call`**: This method defines the operations or transformations applied to the input data as it passes through the layer.\n",
    "\n",
    "- **`process_image`**: The `process_image` method contains the custom logic for loading the image, preprocessing it, generating the text (either a caption or a summary) using the BLIP model, and returning the generated result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fka-W4x4VfTg"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBlipCaptionSummaryLayer\u001b[39;00m(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLayer):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, processor, model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m        Initialize the custom Keras layer with the BLIP processor and model.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m            model: The BLIP model for generating captions or summaries.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class BlipCaptionSummaryLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, processor, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the custom Keras layer with the BLIP processor and model.\n",
    "\n",
    "        Args:\n",
    "            processor: The BLIP processor for preparing inputs for the model.\n",
    "            model: The BLIP model for generating captions or summaries.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "    def call(self, image_path, task):\n",
    "        # Use tf.py_function to run the custom image processing and text generation\n",
    "        return tf.py_function(self.process_image, [image_path, task], tf.string)\n",
    "\n",
    "    def process_image(self, image_path, task):\n",
    "        \"\"\"\n",
    "        Perform image loading, preprocessing, and text generation.\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the image file as a string.\n",
    "            task: The type of task (\"caption\" or \"summary\").\n",
    "\n",
    "        Returns:\n",
    "            The generated caption or summary as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Decode the image path from the TensorFlow tensor to a Python string\n",
    "            image_path_str = image_path.numpy().decode(\"utf-8\")\n",
    "\n",
    "            # Open the image using PIL and convert it to RGB format\n",
    "            image = Image.open(image_path_str).convert(\"RGB\")\n",
    "\n",
    "            # Set the appropriate prompt based on the task\n",
    "            if task.numpy().decode(\"utf-8\") == \"caption\":\n",
    "                prompt = \"This is a picture of\"  # Modify prompt for more natural output\n",
    "            else:\n",
    "                prompt = \"This is a detailed photo showing\"  # Modify for summary\n",
    "\n",
    "            # Prepare inputs for the BLIP model\n",
    "            inputs = self.processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "\n",
    "            # Generate text output using the BLIP model\n",
    "            output = self.model.generate(**inputs)\n",
    "\n",
    "            # Decode the output into a readable string\n",
    "            result = self.processor.decode(output[0], skip_special_tokens=True)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            # Handle errors during image processing or text generation\n",
    "            print(f\"Error: {e}\")\n",
    "            return \"Error processing image\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUDIhtOQVscX"
   },
   "source": [
    "## Task 8: Implement a Helper Function to Use the Custom Keras Layer\n",
    "\n",
    "In this task, you will implement a helper function `generate_text` that utilizes the custom `BlipCaptionSummaryLayer` Keras layer to generate captions or summaries for a given image. The function will accept an image path and a task type (caption or summary), process the image using the BLIP model, and return the generated text.\n",
    "\n",
    "### **Steps:**\n",
    "\n",
    "#### Create the Helper Function `generate_text`:\n",
    "The function will accept following parameters:\n",
    "\n",
    "* **`image_path`**: The path to the image file (in tensor format).\n",
    "* **`task`**: The type of task to perform, which can either be \"caption\" or \"summary\" (in tensor format).\n",
    "  \n",
    "Inside the function:\n",
    "1. Create an instance(blip_layer) of the `BlipCaptionSummaryLayer`.\n",
    "2. Call this layer with the provided image path and task type.\n",
    "3. Return the generated caption or summary as the output.\n",
    "\n",
    "**Note: Please copy and save the code of the task as it will be required for submission in the final project. Ensure to submit the response as part of your project submission**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "pxGPOWXAVxIX"
   },
   "outputs": [],
   "source": [
    "def generate_text(image_path, task):\n",
    "  blip_layer = BlipCaptionSummaryLayer(processor, model)\n",
    "  generated_text = blip_layer(image_path, task)\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3Hgksg2sVxiX"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Path to an example image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m image_path \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maircraft_damage_dataset_v1/test/dent/144_10_JPG_jpg.rf.4d008cc33e217c1606b76585469d626b.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# actual path of image\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Generate a caption for the image\u001b[39;00m\n\u001b[0;32m      5\u001b[0m caption \u001b[38;5;241m=\u001b[39m generate_text(image_path, tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to an example image\n",
    "image_path = tf.constant(\"aircraft_damage_dataset_v1/test/dent/144_10_JPG_jpg.rf.4d008cc33e217c1606b76585469d626b.jpg\")  # actual path of image\n",
    "\n",
    "# Generate a caption for the image\n",
    "caption = generate_text(image_path, tf.constant(\"caption\"))\n",
    "# Decode and print the generated caption\n",
    "print(\"Caption:\", caption.numpy().decode(\"utf-8\"))\n",
    "\n",
    "# Generate a summary for the image\n",
    "summary = generate_text(image_path, tf.constant(\"summary\"))\n",
    "# Decode and print the generated summary\n",
    "print(\"Summary:\", summary.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqLy6VUQXcXr"
   },
   "source": [
    "## Task 9: Generate a caption for an image using the using BLIP pretrained model\n",
    "\n",
    "- Use the image_path variable given below to load the image. Run the cell to before proceeding to next step.\n",
    "- Use the `generate_text` function to generate a caption for the image.\n",
    "- Use the example given in `2.2 Generating Captions and Summaries` for this task\n",
    "  \n",
    "**Note:** Generated captions may not always be accurate, as the model is limited by its training data and may not fully understand new or specific images.\n",
    "  \n",
    "**Note: Please copy and save the output of the task as it will be required for submission in the final project. Ensure to submit the response as part of your project submission**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "EDGDHzYzXSIu"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m image_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load and display the image\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m img \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimread(image_url)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Hide the axis\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# We will use the following image to display Caption and Summary for Task 9 and 10\n",
    "# URL of the image\n",
    "image_url = \"aircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\"\n",
    "# Load and display the image\n",
    "img = plt.imread(image_url)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "h7c3evJdXV5K"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "image_path = tf.constant(\"aircraft_damage_dataset_v1/test/dent/149_22_JPG_jpg.rf.4899cbb6f4aad9588fa3811bb886c34d.jpg\")  # actual path of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "rG-ttfD9XWSk"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate a caption for the image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m caption \u001b[38;5;241m=\u001b[39m generate_text(image_path, tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Decode and print the generated caption\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCaption:\u001b[39m\u001b[38;5;124m\"\u001b[39m, caption\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate a caption for the image\n",
    "caption = generate_text(image_path, tf.constant(\"caption\"))\n",
    "# Decode and print the generated caption\n",
    "print(\"Caption:\", caption.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SVbwvJlXgQ2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
